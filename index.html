---
layout: page
description: ""
---

<div class="post-preview">
		<div class="media">
			<a name="fsaf" class="pull-left">
				<img class="media-object" src="/img/me.jpeg" width="180px" height="180px">
			</a>
			<div class="media-body">
				<p class="media-heading">
					Hello! I am a Research Scientist in NVIDIA Research, working with <a target="_blank"
					   href="https://hanlab.mit.edu/songhan/"><font color="DodgerBlue">Prof. Song Han</font></a>. I got my Ph.D degree in CUHK, supervised by <a target="_blank"
					   href="http://jiaya.me/"><font color="DodgerBlue">Prof. Jiaya Jia</font></a>. Before that, I got M.Phil. in Institute of Automation, CAS, supervised by <a target="_blank"
					 href="https://scholar.google.com/citations?user=5hti_r0AAAAJ&hl=zh-CN"><font color="DodgerBlue"> Prof. Gaofeng Meng</font></a></font>.
			My research interests mainly lie in Efficient Deep Learning, Large Language Models, and Computer Vision. I also worked closely with <a target="_blank" href="https://xjqi.github.io"><font color="DodgerBlue">Prof. Xiaojuan Qi</font></a>,
<a target="_blank" href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=zh-CN"><font color="DodgerBlue">Xiangyu Zhang</font></a>, and
<a target="_blank" href="https://www.taokong.org"><font color="DodgerBlue">Tao Kong</font></a>.
			To know more about me at 
					<a target="_blank"
					   href="https://scholar.google.com/citations?user=6p0ygKUAAAAJ&hl=en"><font color="DodgerBlue">Google Scholar</font></a> 
					and
					<a target="_blank"
					   href="https://github.com/yukang2017"><font color="DodgerBlue">Github</font></a>.
					Any discussion is welcome via <a target="_blank"
					   href="chenyukang2020@gmail.com"><font color="DodgerBlue">E-mail</font></a>.</font>
				</p>
			</div>
		</div>
	</div>


<hr>

<div class="post-preview">
	<strong><font size="6" color="Black">News</font></strong>
		<div class="media">
			<div class="media-body">
				</p>

				<p class="media-heading">
				<strong>
					[2024/01] <a target="_blank"
					   href="https://github.com/dvlab-research/LongLoRA">LongLoRA</font></a> has been accepted by ICLR 2024 as an Oral presentation</strong>.
				</p>
				<p class="media-heading">
				<strong>
					[2023/08] We release <a target="_blank"
					   href="https://github.com/dvlab-research/LongLoRA">LongLoRA</font></a> </strong>, an efficient fine-tuning approach for long-context large language models. We also release <strong> LongAlpaca </strong>, this is the long instruction following dataset, <strong> <a target="_blank" href="https://huggingface.co/datasets/Yukang/LongAlpaca-12k">LongAlpaca-12k</font></a> </strong>, and the corresponding models, <strong> <a target="_blank" href="https://huggingface.co/Yukang/LongAlpaca-70B">LongAlpaca-70B</font></a> </strong>.
				</p>
				<p class="media-heading">
				<strong>
					[2023/08] We release <a target="_blank"
					   href="https://github.com/dvlab-research/LISA"><font color="DodgerBlue">LISA</font></a> </strong>, Reasoning Segmentation via Large Language Model.
				</p>
				<p class="media-heading">
				<strong>
					[2023/07] We release <a target="_blank"
					   href="https://github.com/CVMI-Lab/IST-Net"><font color="DodgerBlue">IST-Net</font></a> </strong>, this is a prior-free pose estimator with SOTA performance and efficiency.
				</p>
				<p class="media-heading">
				<strong>
					[2023/07] Three papers accepted by ICCV 2023, <a target="_blank"
					   href="https://github.com/CVMI-Lab/IST-Net"><font color="DodgerBlue">IST-Net</font></a> </strong>, FocalFormer3D for 3D ojbect detection and tracking, and Accelerated DETR for 3D instance segmentation.
				</p>
				<p class="media-heading">
				<strong>
					[2023/04] We release <a target="_blank"
					   href="https://github.com/dvlab-research/3D-Box-Segment-Anything"><font color="DodgerBlue">3D-Box-Segment-Anything</font></a> </strong>. We extend <a target="_blank" href="https://github.com/facebookresearch/segment-anything"><font color="DodgerBlue">Segment Anything</font></a> to 3D world by combining it and <a target="_blank"
					   href="https://arxiv.org/abs/2303.11301"><font color="DodgerBlue">VoxelNeXt</font></a>. When we provide a prompt (e.g., a point / box), the result is not only 2D segmentation mask, but also 3D boxes.
				</p>
				<p class="media-heading">
				<strong>
					[2023/03] <a target="_blank"
					   href="https://arxiv.org/abs/2303.11301"><font color="DodgerBlue">VoxelNeXt</font></a> is accepted by CVPR 2023 </strong>, a fully sparse VoxelNet.
					It achieves SOTA on Argoverse2 3D object detection and nuScenes LiDAR mutli-object tracking <a target="_blank"
					   href="https://github.com/dvlab-research/VoxelNeXt"><font color="DodgerBlue">Code</font></a>. <strong> VoxelNeXt has been merged into the official <a target="_blank" href="https://github.com/open-mmlab/OpenPCDet"><font color="DodgerBlue">OpenPCDet</font></a> codebase.</strong>
				</p>
				<p class="media-heading">
				<strong>
					[2023/03] <a target="_blank"
					   href="https://arxiv.org/abs/2303.12766"><font color="DodgerBlue">SphereFormer</font></a> is accepted by CVPR 2023 </strong>, spherical window 3D transformer backbone.
					It achieves SOTA on SemanticKITTI and nuScenes LiDAR semantic segmentation <a target="_blank"
					   href="https://github.com/dvlab-research/SphereFormer"><font color="DodgerBlue">Code</font></a>.
				</p>
				<p class="media-heading">
				<strong>
					[2023/03] We release <a target="_blank"
					   href="https://github.com/dvlab-research/SparseTransformer"><font color="DodgerBlue">SparseTransformer</font></a> </strong>, a fast and memory-efficient libarary for sparse transformer for 3D point cloud. This project is lead by <a target="_blank"
					   href="https://scholar.google.com/citations?user=tqNDPA4AAAAJ&hl=zh-CN"><font color="DodgerBlue">Xin Lai</font></a>.
				</p>
				<p class="media-heading">
				<strong>
					[2023/03] 3 Papers accepted by CVPR 2023!
				 </strong>
				</p>
				<p class="media-heading">
				<strong>
					[2022/12] We release <a target="_blank"
					   href="https://github.com/dvlab-research/spconv-plus"><font color="DodgerBlue">spconv-plus</font></a> </strong>, a library based on spconv and integrate several new sparse convolution types and operators that might be useful into this library.
				</p>
				<p class="media-heading">
				<strong>
					[2022/09] <a target="_blank"
					   href="https://arxiv.org/abs/2209.14201"><font color="DodgerBlue">Spatial Pruned Sparse Conv</font></a> is accepted by NeurIPS 2022 </strong>, 50% FLOPs saving for efficient 3D object detection <a target="_blank"
					   href="https://github.com/CVMI-Lab/SPS-Conv"><font color="DodgerBlue">Code</font></a>.
				</p>
				<p class="media-heading">
				<strong>
					[2022/06] <a target="_blank"
					   href="https://arxiv.org/abs/2206.10555"><font color="DodgerBlue">LargeKernel3D</font></a> 1st NDS on nuScenes Leaderboard </strong>, the first large-kernel 3D sparse CNN backbone <a target="_blank"
					   href="https://github.com/dvlab-research/LargeKernel3D"><font color="DodgerBlue">Code</font></a>.
				</p>
				<p class="media-heading">
				<strong>
					[2022/03] <a target="_blank"
					   href="https://arxiv.org/abs/2204.12463"><font color="DodgerBlue">Focal Sparse Conv</font></a> is accepted by CVPR 2022 Oral </strong>, a dynamic sparse convolution for high performance 3D object detection <a target="_blank"
					   href="https://github.com/dvlab-research/FocalsConv"><font color="DodgerBlue">Code</font></a>. <strong> Focal Sparse Conv has been merged into the official <a target="_blank" href="https://github.com/open-mmlab/OpenPCDet"><font color="DodgerBlue">OpenPCDet</font></a> codebase.</strong>
				</p>
				<p class="media-heading">
					<strong>
					[2022/03] <a target="_blank"
					   href="https://ieeexplore.ieee.org/abstract/document/9756374"><font color="DodgerBlue">Scale-aware AutoAug</font></a> is accepted by T-PAMI</strong>, a dynamic training strategy for journal extension <a target="_blank"
					   href="https://github.com/dvlab-research/SA-AutoAug"><font color="DodgerBlue">Code</font></a>.
				 
				</p>
				<p class="media-heading">
					<strong>
					[2021/03] <a target="_blank"
					   href="https://arxiv.org/abs/2103.17220"><font color="DodgerBlue">Scale-aware AutoAug</font></a> is accepted by CVPR 2021</strong>, scale-aware augmentations with automatic augmentation <a target="_blank"
					   href="https://github.com/dvlab-research/SA-AutoAug"><font color="DodgerBlue">Code</font></a>.
				</p>
				<p class="media-heading">
				<strong>
					[2019/12] <a target="_blank"
					   href="https://proceedings.neurips.cc/paper/2019/file/228b25587479f2fc7570428e8bcbabdc-Paper.pdf"><font color="DodgerBlue">DetNAS</font></a> is accepted by NeurIPS 2019 </strong>, the first work that introduces NAS into object detection <a target="_blank"
					   href="https://github.com/megvii-model/DetNAS"><font color="DodgerBlue">Code</font></a>.
				</p>
				<p class="media-heading">
				<strong>
					[2019/10] Winner of Microsoft COCO 2019 Instance Segmentation Track.
				 </strong>
				</p>
				<p class="media-heading">
				<strong>
					[2019/12] <a target="_blank"
					   href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_RENAS_Reinforced_Evolutionary_Neural_Architecture_Search_CVPR_2019_paper.pdf"><font color="DodgerBlue">RENAS</font></a> is accepted by CVPR 2019 </strong>, the first work that combine reinforcement learning and evolutionary search in NAS <a target="_blank"
					   href="https://github.com/yukang2017/RENAS"><font color="DodgerBlue">Code</font></a>.
				</p>
			</div>
		</div>
</div>

<hr>
<div class="post-preview">
	<strong><font size="6" color="Black">Education</font></strong>
		<div class="media">
			<a name="fsaf" class="pull-left">
				<img class="media-object" src="/img/cuhk.jpeg" width="50px" height="50px">
			</a>
			<div class="media-body">
				<p class="media-heading">
					<strong>
						 The Chinese University of Hong Kong
				 </strong> <br>
					<font size="2.7">PhD., Computer Science and Engineering.</font>
					<font size="2.7">Aug 2020 - Jul 2024.</font> <br>
					<font size="2.7">Supervisor: Prof. <a target="_blank"
					   href="http://jiaya.me/"><font color="DodgerBlue">Jiaya Jia</font>.</a> <br>

				</p>
			</div>
		</div>
		<div class="media">
			<a name="fsaf" class="pull-left">
				<img class="media-object" src="/img/casia.jpeg" width="50px" height="48px">
			</a>
			<div class="media-body">
				<p class="media-heading">
					<strong>
						 Institute of Automation, Chinese Academy of Sciences
				 </strong><br>
					<font size="2.7">M.Phil., Pattern Recognition and Intelligent System.</font>
					<font size="2.7">Sep 2017 - Jul 2020.</font> <br>
					<font size="2.7">Supervisor: Prof. <a target="_blank"
					   href="https://scholar.google.com/citations?user=5hti_r0AAAAJ&hl=zh-CN"><font color="DodgerBlue">Gaofeng Meng</font></a></font> <font size="2.7">  and Prof. <a target="_blank"
					   href="https://scholar.google.com/citations?user=0ggsACEAAAAJ&hl=zh-CN"><font color="DodgerBlue">Shiming Xiang</font></a>. <br>
				</p>
			</div>
		</div>
		<div class="media">
			<a name="fsaf" class="pull-left">
				<img class="media-object" src="/img/beihang.jpeg" width="50px" height="50px">
			</a>
			<div class="media-body">
				<p class="media-heading">
					<strong>
						 Beihang University
				 </strong><br>
					<font size="2.7">B.Eng., Major in Guide Naviation and Control, Sep 2013 - Jul 2017. </font><br>
				</p>
			</div>
		</div>

</div>
<hr>

<div class="post-preview">
	<strong><font size="6" color="Black">Activities</font></strong>
		<div class="media">
			<div class="media-body">
				<p class="media-heading">
					<strong>
						 Reviewer
				 </strong><br>
					<font size="2.7"> CVPR, ICCV, ECCV, AAAI, NeurIPS, T-PAMI, Pattern Recognition.</font><br>
				</p>
			</div>
		<div class="media">
			<div class="media-body">
				<p class="media-heading">
					<strong>
						 Teaching Assistant
				 </strong><br>
					<font size="2.7"> CSCI 3260 Principles of Computer Graphics </font><br>
					<font size="2.7"> ENGG 5104 Image Processing and Computer Vision </font>
					<br>
				</p>
			</div>
		</div>
</div>

